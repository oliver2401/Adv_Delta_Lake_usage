{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, expr, rand, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DeltaLake Transaction Logs\") \\\n",
    "    .master(\"local[3]\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Main class for programmatically intereacting with Delta Tables.\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 4)\n",
    "spark.conf.set(\"spark.default.parallelism\", 4)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "ERROR:root:File `'Helpers.ipynb.py'` not found.\n"
    }
   ],
   "source": [
    "%run Helpers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Deleted path /home/oliver/DeltaLake/training/online_retail_dataset/delta/online_retail_data\nDeleted path /home/oliver/DeltaLake/training/online_retail_dataset/parquet/online_retail_data\n"
    }
   ],
   "source": [
    "# Source data\n",
    "# Change paths to match your environment!\n",
    "\n",
    "inputPath    = \"/home/oliver/DeltaLake/training/online_retail_dataset/CSV_DATA/\"\n",
    "sourceData   = inputPath + \"online-retail-dataset.csv\"\n",
    "\n",
    "# Base location for all saved data\n",
    "basePath     = \"/home/oliver/DeltaLake/training/online_retail_dataset\" \n",
    "\n",
    "# Path for Parquet formatted data\n",
    "parquetPath  = basePath + \"/parquet/online_retail_data\"\n",
    "\n",
    "# Path for Delta formatted data\n",
    "deltaPath    = basePath + \"/delta/online_retail_data\"\n",
    "deltaLogPath = deltaPath + \"/_delta_log\"\n",
    "\n",
    "# Clean up from last run.\n",
    "! rm -Rf $deltaPath 2>/dev/null\n",
    "print(\"Deleted path \" + deltaPath)\n",
    "\n",
    "! rm -Rf $parquetPath 2>/dev/null\n",
    "print(\"Deleted path \" + parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Dataset is present.\n\nFile [/home/oliver/DeltaLake/training/online_retail_dataset/CSV_DATA/online-retail-dataset.csv] is ['43'] MB in size.\n"
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "file_exists = os.path.isfile(f'{sourceData}')\n",
    " \n",
    "if not file_exists:\n",
    "    print(\"-> Downloading dataset.\")\n",
    "    os.system(f'curl https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/all/online-retail-dataset.csv -o {sourceData}')\n",
    "    file_exists = os.path.isfile(f'{sourceData}')\n",
    "    \n",
    "if file_exists:\n",
    "    print(\"-> Dataset is present.\\n\")\n",
    "    \n",
    "    fileSize = ! du -m \"$sourceData\" | cut -f1 # Posix compliant\n",
    "    print(f\"File [{sourceData}] is {fileSize} MB in size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n"
    }
   ],
   "source": [
    "! head -n 5 $sourceData 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaDDL = \"\"\"InvoiceNo Integer, StockCode String, Description String, Quantity Integer, \n",
    "               InvoiceDate String, UnitPrice Double, CustomerID Integer, Country String \"\"\"\n",
    "\n",
    "\n",
    "# You could also use the StructType method.\n",
    "# Libraries needed to define schemas\n",
    "# from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "\n",
    "#inputSchema = StructType([\n",
    "#  StructField(\"InvoiceNo\", IntegerType(), True),\n",
    "#  StructField(\"StockCode\", StringType(), True),\n",
    "#  StructField(\"Description\", StringType(), True),\n",
    "#  StructField(\"Quantity\", IntegerType(), True),\n",
    "#  StructField(\"InvoiceDate\", StringType(), True),\n",
    "#  StructField(\"UnitPrice\", DoubleType(), True),\n",
    "#  StructField(\"CustomerID\", IntegerType(), True),\n",
    "#  StructField(\"Country\", StringType(), True)\n",
    "#])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Row Count: 541909 Partition Count: 3\n"
    }
   ],
   "source": [
    "# Create retail sales data dataframe\n",
    "\n",
    "rawSalesDataDF = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .schema(schemaDDL)\n",
    "    .load(sourceData)\n",
    ")\n",
    "\n",
    "# Count rows and partitions\n",
    "rowCount = rawSalesDataDF.count() \n",
    "partCount = rawSalesDataDF.rdd.getNumPartitions()\n",
    "\n",
    "print(f'Row Count: {rowCount} Partition Count: {partCount}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Columns with null values\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n|     9291|        0|       1454|       0|          0|        0|    135080|      0|\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n\n"
    }
   ],
   "source": [
    "print(\"Columns with null values\")\n",
    "rawSalesDataDF.select([count(when(col(c).isNull(), c)).alias(c) for c in rawSalesDataDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "null values\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n|        0|        0|          0|       0|          0|        0|         0|      0|\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n\n RowsRemoved: 143985\n Final Row Count: 397924\n"
    }
   ],
   "source": [
    "# Remove rows where important columns are null. In our case: InvoiceNo and CustomerID\n",
    "\n",
    "cleanSalesDataDF = rawSalesDataDF.where(col(\"InvoiceNo\").isNotNull() & col(\"CustomerID\").isNotNull())\n",
    "cleanSalesDataCount = cleanSalesDataDF.count()\n",
    "# POO cleanSalesDataDF = cleanSalesDataDF.where(col(\"CustomerID\").isNotNull())\n",
    "\n",
    "# All rows with null values should be gone\n",
    "print(\"null values\")\n",
    "cleanSalesDataDF.select([count(when(col(c).isNull(), c)).alias(c) for c in rawSalesDataDF.columns]).show()\n",
    "\n",
    "print(f' RowsRemoved: {rowCount-cleanSalesDataCount}\\n Final Row Count: {cleanSalesDataCount}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Row Count: 99057 Partition Count: 3\n"
    }
   ],
   "source": [
    "# Define new dataframe based on cleansed data but only use a subset of the data to make things run faster\n",
    "\n",
    "# Random sample of 25%, with seed and without replacement\n",
    "retailSalesData1 = cleanSalesDataDF.sample(withReplacement=False, fraction=.25, seed=75)\n",
    "\n",
    "# Count rows and partitions\n",
    "rowCount = retailSalesData1.count() \n",
    "partCount = retailSalesData1.rdd.getNumPartitions()\n",
    "\n",
    "print(f'Row Count: {rowCount} Partition Count: {partCount}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n|536365   |21730    |GLASS STAR FROSTED T-LIGHT HOLDER  |6       |12/1/2010 8:26|4.25     |17850     |United Kingdom|\n+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\nonly showing top 3 rows\n\n"
    }
   ],
   "source": [
    "# Peek at the dataframe\n",
    "\n",
    "retailSalesData1.show(3, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+\n|namespace|\n+---------+\n|  default|\n|deltademo|\n+---------+\n\n+------------------+\n|current_database()|\n+------------------+\n|         deltademo|\n+------------------+\n\n+-------------------------+-------------------------------------------------------------------+\n|database_description_item|database_description_value                                         |\n+-------------------------+-------------------------------------------------------------------+\n|Database Name            |deltademo                                                          |\n|Comment                  |Esta es una DB demo para uso del formato Delta                     |\n|Location                 |file:/home/oliver/DeltaLake/training/online_retail_dataset/DATABASE|\n|Owner                    |oliver                                                             |\n+-------------------------+-------------------------------------------------------------------+\n\n+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n+--------+---------+-----------+\n\n"
    }
   ],
   "source": [
    "# Create database to hold demo objects\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS deltademo COMMENT 'Esta es una DB demo para uso del formato Delta' LOCATION '/home/oliver/DeltaLake/training/online_retail_dataset/DATABASE'\")\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "# Current DB should be deltademo\n",
    "spark.sql(\"USE deltademo\")\n",
    "spark.sql(\"SELECT CURRENT_DATABASE()\").show()\n",
    "spark.sql(\"DESCRIBE DATABASE deltademo\").show(truncate = False)\n",
    "\n",
    "# Clean-up from last run\n",
    "spark.sql(\"DROP TABLE IF EXISTS SalesParquetFormat\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS SalesDeltaFormat\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS tbl_CheckpointFile\")\n",
    "spark.sql(\"SHOW TABLES\").show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+\n|namespace|\n+---------+\n|  default|\n+---------+\n\n"
    }
   ],
   "source": [
    "#spark.sql(\"DROP DATABASE IF EXISTS deltademo\")\n",
    "#spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as a table in Parquet format\n",
    "\n",
    "retailSalesData1.write.saveAsTable('SalesParquetFormat', format='parquet', mode='overwrite',path=parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Table(name='salesparquetformat', database='deltademo', description=None, tableType='EXTERNAL', isTemporary=False)]"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Let's peek into the catalog and verify that our table was created\n",
    "\n",
    "spark.catalog.listTables()\n",
    "\n",
    "# SQL method - not as informative\n",
    "# spark.sql(\"show tables\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "279KB    2020-11-11 22:28:52  part-00002-687c91cd-1dab-4ce0-abaf-f1d1c63e08b5-c000.snappy.parquet\n399KB    2020-11-11 22:28:52  part-00001-687c91cd-1dab-4ce0-abaf-f1d1c63e08b5-c000.snappy.parquet\n379KB    2020-11-11 22:28:52  part-00000-687c91cd-1dab-4ce0-abaf-f1d1c63e08b5-c000.snappy.parquet\n\nNumber of file/s: 3 | Total size: 1.1M\n"
    }
   ],
   "source": [
    "# Files and size on disk\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "def get_printable_size(byte_size):\n",
    "    \"\"\"\n",
    "    A bit is the smallest unit, it's either 0 or 1\n",
    "    1 byte = 1 octet = 8 bits\n",
    "    1 kB = 1 kilobyte = 1000 bytes = 10^3 bytes\n",
    "    1 KiB = 1 kibibyte = 1024 bytes = 2^10 bytes\n",
    "    1 KB = 1 kibibyte OR kilobyte ~= 1024 bytes ~= 2^10 bytes (it usually means 1024 bytes but sometimes it's 1000... ask the sysadmin ;) )\n",
    "    1 kb = 1 kilobits = 1000 bits (this notation should not be used, as it is very confusing)\n",
    "    1 ko = 1 kilooctet = 1000 octets = 1000 bytes = 1 kB\n",
    "    Also Kb seems to be a mix of KB and kb, again it depends on context.\n",
    "    In linux, a byte (B) is composed by a sequence of bits (b). One byte has 256 possible values.\n",
    "    More info : http://www.linfo.org/byte.html\n",
    "    \"\"\"\n",
    "    BASE_SIZE = 1024.00\n",
    "    MEASURE = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n",
    "\n",
    "    def _fix_size(size, size_index):\n",
    "        if not size:\n",
    "            return \"0\"\n",
    "        elif size_index == 0:\n",
    "            return str(size)\n",
    "        else:\n",
    "            return \"{:.0f}\".format(size)\n",
    "\n",
    "    current_size = byte_size\n",
    "    size_index = 0\n",
    "\n",
    "    while current_size >= BASE_SIZE and len(MEASURE) != size_index:\n",
    "        current_size = current_size / BASE_SIZE\n",
    "        size_index = size_index + 1\n",
    "\n",
    "    size = _fix_size(current_size, size_index)\n",
    "    measure = MEASURE[size_index]\n",
    "    return size + measure\n",
    "    \n",
    "def files_in_dir(dir_path, file_ext):\n",
    "    files = glob.glob(f'{dir_path}/*.{file_ext}')\n",
    "    files.sort(key=os.path.getmtime)\n",
    "    \n",
    "    for f in files:\n",
    "        x = os.path.getmtime(f)\n",
    "        x = datetime.datetime.fromtimestamp(x).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f'{get_printable_size(os.path.getsize(f)):8} {x}  {os.path.basename(f)}')\n",
    "\n",
    "    numFiles = os.popen(f'ls -p {dir_path}/*.{file_ext} | egrep -v /$ | wc -l').read().strip()\n",
    "    totalSize = os.popen(f'! du -sh {dir_path} | cut -f1').read().strip()\n",
    "    \n",
    "    print(\"\")\n",
    "    print(f\"Number of file/s: {numFiles} | Total size: {totalSize}\")\n",
    "\n",
    "files_in_dir(parquetPath, \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----------------------------+-------------------------------------------------------------------------------------+-------+\n|col_name                    |data_type                                                                            |comment|\n+----------------------------+-------------------------------------------------------------------------------------+-------+\n|InvoiceNo                   |int                                                                                  |null   |\n|StockCode                   |string                                                                               |null   |\n|Description                 |string                                                                               |null   |\n|Quantity                    |int                                                                                  |null   |\n|InvoiceDate                 |string                                                                               |null   |\n|UnitPrice                   |double                                                                               |null   |\n|CustomerID                  |int                                                                                  |null   |\n|Country                     |string                                                                               |null   |\n|                            |                                                                                     |       |\n|# Detailed Table Information|                                                                                     |       |\n|Database                    |deltademo                                                                            |       |\n|Table                       |salesparquetformat                                                                   |       |\n|Owner                       |oliver                                                                               |       |\n|Created Time                |Wed Nov 11 22:28:52 CST 2020                                                         |       |\n|Last Access                 |UNKNOWN                                                                              |       |\n|Created By                  |Spark 3.0.0                                                                          |       |\n|Type                        |EXTERNAL                                                                             |       |\n|Provider                    |parquet                                                                              |       |\n|Statistics                  |1083137 bytes                                                                        |       |\n|Location                    |file:/home/oliver/DeltaLake/training/online_retail_dataset/parquet/online_retail_data|       |\n|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                          |       |\n|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                        |       |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                       |       |\n+----------------------------+-------------------------------------------------------------------------------------+-------+\n\n"
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED SalesParquetFormat\").show(100,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+---------+-----------------------------------+--------+---------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate    |UnitPrice|CustomerID|Country       |\n+---------+---------+-----------------------------------+--------+---------------+---------+----------+--------------+\n|554055   |22469    |HEART OF WICKER SMALL              |40      |5/20/2011 15:39|1.45     |18198     |United Kingdom|\n|554057   |18097C   |WHITE TALL PORCELAIN T-LIGHT HOLDER|288     |5/20/2011 15:52|1.95     |17867     |United Kingdom|\n|554057   |72351B   |SET/6 PINK  BUTTERFLY T-LIGHTS     |1       |5/20/2011 15:52|2.1      |17867     |United Kingdom|\n+---------+---------+-----------------------------------+--------+---------------+---------+----------+--------------+\nonly showing top 3 rows\n\n"
    }
   ],
   "source": [
    "# Use Spark SQL to query the newly created table\n",
    "\n",
    "spark.sql(\"SELECT * FROM SalesParquetFormat;\").show(3, truncate = False)\n",
    "\n",
    "# You can directly query the directory too.\n",
    "# spark.sql(f\"SELECT * FROM parquet.`{parquetPath}` limit 5 \").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[]"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Add one row of data to the table\n",
    "# Parquet being immutable necessitates the creation of an additional Parquet file\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            INSERT INTO SalesParquetFormat\n",
    "            VALUES(963316, 2291, \"WORLD'S BEST JAM MAKING SET\", 5 , \"08/13/2011 07:58\", 1.45, 15358, \"United Kingdom\")\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "279KB    2020-11-11 22:28:52  part-00002-687c91cd-1dab-4ce0-abaf-f1d1c63e08b5-c000.snappy.parquet\n399KB    2020-11-11 22:28:52  part-00001-687c91cd-1dab-4ce0-abaf-f1d1c63e08b5-c000.snappy.parquet\n379KB    2020-11-11 22:28:52  part-00000-687c91cd-1dab-4ce0-abaf-f1d1c63e08b5-c000.snappy.parquet\n2KB      2020-11-11 22:30:36  part-00000-3409adc9-47a2-491e-850e-4e3e7137057c-c000.snappy.parquet\n\nNumber of file/s: 4 | Total size: 1.1M\n"
    }
   ],
   "source": [
    "files_in_dir(parquetPath, \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+---------+---------------------------+--------+----------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                |Quantity|InvoiceDate     |UnitPrice|CustomerID|Country       |\n+---------+---------+---------------------------+--------+----------------+---------+----------+--------------+\n|963316   |2291     |WORLD'S BEST JAM MAKING SET|5       |08/13/2011 07:58|1.45     |15358     |United Kingdom|\n+---------+---------+---------------------------+--------+----------------+---------+----------+--------------+\n\n"
    }
   ],
   "source": [
    "# Let's have a peek at the new file\n",
    "\n",
    "spark.read.load(f\"{parquetPath}/part-00000-3409adc9-47a2-491e-850e-4e3e7137057c-c000.snappy.parquet\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+---------+-----------------------------------+--------+---------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate    |UnitPrice|CustomerID|Country       |\n+---------+---------+-----------------------------------+--------+---------------+---------+----------+--------------+\n|554055   |22469    |HEART OF WICKER SMALL              |40      |5/20/2011 15:39|1.45     |18198     |United Kingdom|\n|554057   |18097C   |WHITE TALL PORCELAIN T-LIGHT HOLDER|288     |5/20/2011 15:52|1.95     |17867     |United Kingdom|\n|554057   |72351B   |SET/6 PINK  BUTTERFLY T-LIGHTS     |1       |5/20/2011 15:52|2.1      |17867     |United Kingdom|\n+---------+---------+-----------------------------------+--------+---------------+---------+----------+--------------+\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[database: string, tableName: string, isTemporary: boolean]"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Save retailSalesData1 to Delta\n",
    "\n",
    "retailSalesData1.write.mode(\"overwrite\").format(\"delta\").save(deltaPath)#aqui solo creó nuevos datos en formato DELTA en el path 'deltaPath'\n",
    "#pero no hizo una nueva tabla\n",
    "\n",
    "# Query delta directory directly\n",
    "spark.sql(f\"SELECT * FROM delta.`{deltaPath}` LIMIT 3 \").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable for a path based Delta table\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, deltaPath) # Aquí solo crea una variable para poder accesar y monitorear los cambios a los archivos DELTA en 'deltaPath'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "####### HISTORY ########\n+---+-------------------+---------+--------------------------------------+------------------------------------------------------------------+\n|ver|timestamp          |operation|operationParameters                   |operationMetrics                                                  |\n+---+-------------------+---------+--------------------------------------+------------------------------------------------------------------+\n|0  |2020-11-11 22:32:17|WRITE    |[mode -> Overwrite, partitionBy -> []]|[numFiles -> 3, numOutputBytes -> 1083137, numOutputRows -> 99057]|\n+---+-------------------+---------+--------------------------------------+------------------------------------------------------------------+\n\n"
    }
   ],
   "source": [
    "print(\"####### HISTORY ########\")\n",
    "\n",
    "# Observe history of actions taken on a Delta table\n",
    "\n",
    "history = deltaTable.history().select('version', 'timestamp', 'operation', 'operationParameters', \\\n",
    "                                      'operationMetrics') \\\n",
    "                              .withColumnRenamed('version', 'ver')\n",
    "history.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "total 1.1M\ndrwxrwxr-x 2 oliver oliver 4.0K Nov 11 22:32 _delta_log\n-rw-r--r-- 1 oliver oliver 400K Nov 11 22:32 part-00001-2088d53e-b24c-43c0-9062-c46c06918ac9-c000.snappy.parquet\n-rw-r--r-- 1 oliver oliver 380K Nov 11 22:32 part-00000-ed1583ac-9038-4ca2-ac09-485e334d4d80-c000.snappy.parquet\n-rw-r--r-- 1 oliver oliver 280K Nov 11 22:32 part-00002-405a1d18-b93c-4062-9069-07fdfdafd884-c000.snappy.parquet\n"
    }
   ],
   "source": [
    "# files and size on disk\n",
    "# Notice the sub-directory \"_delta_log\"\n",
    "\n",
    "! ls -thl $deltaPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2KB      2020-11-11 22:32:17  00000000000000000000.json\n\nNumber of file/s: 1 | Total size: 8.0K\n"
    }
   ],
   "source": [
    "# # files and size on disk\n",
    "# We can see that parquet files were added but now there is a trx log\n",
    "\n",
    "files_in_dir(deltaLogPath, \"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(add=None, commitInfo=Row(isBlindAppend=False, operation='WRITE', operationMetrics=Row(numFiles='3', numOutputBytes='1083137', numOutputRows='99057'), operationParameters=Row(mode='Overwrite', partitionBy='[]'), timestamp=1605132570093), metaData=None, protocol=None),\n Row(add=None, commitInfo=None, metaData=None, protocol=Row(minReaderVersion=1, minWriterVersion=2)),\n Row(add=None, commitInfo=None, metaData=Row(createdTime=1605132568722, format=Row(provider='parquet'), id='9395c7c5-4308-4c77-aec1-7464fc8fe70f', partitionColumns=[], schemaString='{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNo\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StockCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Description\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Quantity\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"InvoiceDate\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"UnitPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerID\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Country\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}'), protocol=None),\n Row(add=Row(dataChange=True, modificationTime=1605132569000, path='part-00000-e09d4d65-b48f-48ff-bb11-32ec31c62321-c000.snappy.parquet', size=388268), commitInfo=None, metaData=None, protocol=None),\n Row(add=Row(dataChange=True, modificationTime=1605132569000, path='part-00001-28fd7101-7f97-4028-83c5-5dcdd9b20754-c000.snappy.parquet', size=408716), commitInfo=None, metaData=None, protocol=None),\n Row(add=Row(dataChange=True, modificationTime=1605132569000, path='part-00002-4f72cf15-cb0b-414d-921d-9aa9abb594fb-c000.snappy.parquet', size=286153), commitInfo=None, metaData=None, protocol=None)]"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# Let's have a peek inside the trx log\n",
    "\n",
    "#spark.read.format(\"json\").load(deltaLogPath + \"/00000000000000000000.json\").collect()\n",
    "spark.read.format(\"json\").load(deltaLogPath + \"/00000000000000000000.json\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "99463"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# Create a new dataframe with fraction of original data.\n",
    "# Random sample of 25%, with seed and without replacement\n",
    "\n",
    "retailSalesData2 = cleanSalesDataDF.sample(withReplacement=False, fraction=.25, seed=31)\n",
    "retailSalesData2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to our Delta Lake table by appending retailSalesData2\n",
    "\n",
    "retailSalesData2.write.mode(\"append\").format(\"delta\").save(deltaPath) # solo agregó más datos a los DELTA files pero todavía no ha creado una tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "####### HISTORY ########\n+---+-------------------+---------+--------------------------------------+------------------------------------------------------------------+\n|ver|timestamp          |operation|operationParameters                   |operationMetrics                                                  |\n+---+-------------------+---------+--------------------------------------+------------------------------------------------------------------+\n|2  |2020-11-11 22:36:42|WRITE    |[mode -> Append, partitionBy -> []]   |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]|\n|1  |2020-11-11 22:35:49|WRITE    |[mode -> Append, partitionBy -> []]   |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]|\n|0  |2020-11-11 22:32:17|WRITE    |[mode -> Overwrite, partitionBy -> []]|[numFiles -> 3, numOutputBytes -> 1083137, numOutputRows -> 99057]|\n+---+-------------------+---------+--------------------------------------+------------------------------------------------------------------+\n\n"
    }
   ],
   "source": [
    "print(\"####### HISTORY ########\")\n",
    "\n",
    "# Observe history of actions taken on a Delta table\n",
    "# Reference for full history schema: https://docs.delta.io/latest/delta-utility.html\n",
    "\n",
    "history = deltaTable.history().select('version', 'timestamp', 'operation', 'operationParameters', \\\n",
    "                                      'operationMetrics') \\\n",
    "                              .withColumnRenamed(\"version\", 'ver')\n",
    "history.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "279KB    2020-11-11 16:09:29  part-00002-4f72cf15-cb0b-414d-921d-9aa9abb594fb-c000.snappy.parquet\n399KB    2020-11-11 16:09:29  part-00001-28fd7101-7f97-4028-83c5-5dcdd9b20754-c000.snappy.parquet\n379KB    2020-11-11 16:09:29  part-00000-e09d4d65-b48f-48ff-bb11-32ec31c62321-c000.snappy.parquet\n281KB    2020-11-11 16:09:59  part-00002-e86f84af-b8c7-46c1-9339-e4e13871b47f-c000.snappy.parquet\n379KB    2020-11-11 16:09:59  part-00000-6862b1f2-21bf-426b-926c-e8c0d3859485-c000.snappy.parquet\n398KB    2020-11-11 16:09:59  part-00001-baff5b51-e2f0-4120-b7b6-56d621641096-c000.snappy.parquet\n\nNumber of file/s: 6 | Total size: 2.2M\n"
    }
   ],
   "source": [
    "# Data Files and size on disk\n",
    "\n",
    "files_in_dir(deltaPath, \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2KB      2020-11-11 16:09:30  00000000000000000000.json\n765B     2020-11-11 16:09:59  00000000000000000001.json\n\nNumber of file/s: 2 | Total size: 12K\n"
    }
   ],
   "source": [
    "# Transaction logs and size on disk\n",
    "\n",
    "files_in_dir(deltaLogPath, \"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(add=None, commitInfo=Row(isBlindAppend=True, operation='WRITE', operationMetrics=Row(numFiles='3', numOutputBytes='1082711', numOutputRows='99463'), operationParameters=Row(mode='Append', partitionBy='[]'), readVersion=0, timestamp=1605132599453)),\n Row(add=Row(dataChange=True, modificationTime=1605132599000, path='part-00000-6862b1f2-21bf-426b-926c-e8c0d3859485-c000.snappy.parquet', size=388155), commitInfo=None),\n Row(add=Row(dataChange=True, modificationTime=1605132599000, path='part-00001-baff5b51-e2f0-4120-b7b6-56d621641096-c000.snappy.parquet', size=407069), commitInfo=None),\n Row(add=Row(dataChange=True, modificationTime=1605132599000, path='part-00002-e86f84af-b8c7-46c1-9339-e4e13871b47f-c000.snappy.parquet', size=287487), commitInfo=None)]"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# Peek inside the new transaction log\n",
    "\n",
    "#logDF = spark.read.format(\"json\").load(deltaLogPath + \"/00000000000000000001.json\")\n",
    "#logDF.collect()\n",
    "\n",
    "logDF = spark.read.format(\"json\").load(deltaLogPath + \"/00000000000000000001.json\")\n",
    "logDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[]"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "# Create SQL table to make life easier\n",
    "# Stick with SQL from here on out, where possible.\n",
    "# En esta parte crea una nueva tabla utilizando los archivos que ya están presentes en el directorio 'delta'\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          DROP TABLE IF EXISTS SalesDeltaFormat\n",
    "          \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          CREATE TABLE SalesDeltaFormat\n",
    "          USING DELTA\n",
    "          LOCATION '{}'\n",
    "          \"\"\".format(deltaPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Table(name='salesdeltaformat', database='deltademo', description=None, tableType='EXTERNAL', isTemporary=False),\n Table(name='salesparquetformat', database='deltademo', description=None, tableType='EXTERNAL', isTemporary=False)]"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "# Let's peek into the catalog and verify that our table was created.\n",
    "#spark.catalog.listTables()\n",
    "\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----------------------------+-----------------------------------------------------------------------------------+-------+\n|col_name                    |data_type                                                                          |comment|\n+----------------------------+-----------------------------------------------------------------------------------+-------+\n|InvoiceNo                   |int                                                                                |       |\n|StockCode                   |string                                                                             |       |\n|Description                 |string                                                                             |       |\n|Quantity                    |int                                                                                |       |\n|InvoiceDate                 |string                                                                             |       |\n|UnitPrice                   |double                                                                             |       |\n|CustomerID                  |int                                                                                |       |\n|Country                     |string                                                                             |       |\n|                            |                                                                                   |       |\n|# Partitioning              |                                                                                   |       |\n|Not partitioned             |                                                                                   |       |\n|                            |                                                                                   |       |\n|# Detailed Table Information|                                                                                   |       |\n|Name                        |deltademo.salesdeltaformat                                                         |       |\n|Location                    |file:/home/oliver/DeltaLake/training/online_retail_dataset/delta/online_retail_data|       |\n|Provider                    |delta                                                                              |       |\n|Table Properties            |[]                                                                                 |       |\n+----------------------------+-----------------------------------------------------------------------------------+-------+\n\n"
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED SalesDeltaFormat\").show(100, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Random Invoice # => 554207\n"
    }
   ],
   "source": [
    "# Let's find a Invoice with only 1 count and use it to test DML.\n",
    "\n",
    "\n",
    "oneRandomInvoice = spark.sql(\"\"\" SELECT InvoiceNo, count(*)\n",
    "                                 FROM SalesDeltaFormat\n",
    "                                 GROUP BY InvoiceNo\n",
    "                                 ORDER BY 2 ASC\n",
    "                                 LIMIT 1\n",
    "                             \"\"\").collect()[0][0]\n",
    "\n",
    "print(f\"Random Invoice # => {oneRandomInvoice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------------------------------------------------------------------+---------+---------+-----------------------------+--------+---------------+---------+----------+--------------+\n|Filename                                                           |InvoiceNo|StockCode|Description                  |Quantity|InvoiceDate    |UnitPrice|CustomerID|Country       |\n+-------------------------------------------------------------------+---------+---------+-----------------------------+--------+---------------+---------+----------+--------------+\n|part-00001-2088d53e-b24c-43c0-9062-c46c06918ac9-c000.snappy.parquet|554207   |85132C   |CHARLIE AND LOLA FIGURES TINS|1       |5/23/2011 12:15|1.95     |14573     |United Kingdom|\n+-------------------------------------------------------------------+---------+---------+-----------------------------+--------+---------------+---------+----------+--------------+\n\n"
    }
   ],
   "source": [
    "# Before DML (insert)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "                SELECT SUBSTRING(input_file_name(), -67, 67 ) AS Filename,\n",
    "                *\n",
    "                FROM SalesDeltaFormat\n",
    "                WHERE InvoiceNo = {oneRandomInvoice}\n",
    "           \"\"\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[]"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "# Let's add some data to our table\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "               INSERT INTO SalesDeltaFormat\n",
    "               VALUES({oneRandomInvoice}, 2291, \"WORLD'S BEST JAM MAKING SET\", 5, \"08/13/2011 07:58\", 1.45, 15358, \"France\");\n",
    "           \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------+-------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------------+------------+\n|version|timestamp          |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                  |userMetadata|\n+-------+-------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------------+------------+\n|3      |2020-11-11 22:40:01|null  |null    |WRITE    |[mode -> Append, partitionBy -> []]   |null|null    |null     |2          |null          |true         |[numFiles -> 1, numOutputBytes -> 2369, numOutputRows -> 1]       |null        |\n|2      |2020-11-11 22:36:42|null  |null    |WRITE    |[mode -> Append, partitionBy -> []]   |null|null    |null     |1          |null          |true         |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]|null        |\n|1      |2020-11-11 22:35:49|null  |null    |WRITE    |[mode -> Append, partitionBy -> []]   |null|null    |null     |0          |null          |true         |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]|null        |\n|0      |2020-11-11 22:32:17|null  |null    |WRITE    |[mode -> Overwrite, partitionBy -> []]|null|null    |null     |null       |null          |false        |[numFiles -> 3, numOutputBytes -> 1083137, numOutputRows -> 99057]|null        |\n+-------+-------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------------+------------+\n\n"
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE HISTORY SalesDeltaFormat\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2KB      2020-11-11 16:09:30  00000000000000000000.json\n765B     2020-11-11 16:09:59  00000000000000000001.json\n410B     2020-11-11 16:10:31  00000000000000000002.json\n\nNumber of file/s: 3 | Total size: 16K\n"
    }
   ],
   "source": [
    "files_in_dir(deltaLogPath,\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(add=None, commitInfo=Row(isBlindAppend=True, operation='WRITE', operationMetrics=Row(numFiles='3', numOutputBytes='1082711', numOutputRows='99463'), operationParameters=Row(mode='Append', partitionBy='[]'), readVersion=1, timestamp=1605155802911)),\n Row(add=Row(dataChange=True, modificationTime=1605155802000, path='part-00000-8bad6924-194f-4f7c-be73-1a5d00ff94bc-c000.snappy.parquet', size=388155), commitInfo=None),\n Row(add=Row(dataChange=True, modificationTime=1605155802000, path='part-00001-2576e8c3-d158-4d3f-a1f5-3ba1debe407e-c000.snappy.parquet', size=407069), commitInfo=None),\n Row(add=Row(dataChange=True, modificationTime=1605155802000, path='part-00002-172a1fcd-26b8-4489-94fd-e243d09d4d6f-c000.snappy.parquet', size=287487), commitInfo=None)]"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "# Schema details: https://docs.delta.io/latest/delta-utility.html\n",
    "\n",
    "logDF = spark.read.format(\"json\").load(deltaLogPath + \"/00000000000000000002.json\")\n",
    "logDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------------------------------------------------------------------+---------+---------+----------------------------------+--------+----------------+---------+----------+--------------+\n|FileName                                                           |InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate     |UnitPrice|CustomerID|Country       |\n+-------------------------------------------------------------------+---------+---------+----------------------------------+--------+----------------+---------+----------+--------------+\n|part-00001-28fd7101-7f97-4028-83c5-5dcdd9b20754-c000.snappy.parquet|554485   |23152    |IVORY SWEETHEART WIRE LETTER RACK |2       |5/24/2011 13:22 |3.75     |18018     |United Kingdom|\n|part-00000-b783e06e-a2cc-4d2d-ab2f-ba4bbe61552a-c000.snappy.parquet|554485   |2291     |WORLD'S BEST JAM MAKING SET       |5       |08/13/2011 07:58|1.45     |15358     |France        |\n+-------------------------------------------------------------------+---------+---------+----------------------------------+--------+----------------+---------+----------+--------------+\n\n"
    }
   ],
   "source": [
    "# After DML (insert)\n",
    "\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "               SELECT SUBSTRING(input_file_name(), -67, 67) AS FileName, *\n",
    "               FROM SalesDeltaFormat\n",
    "               WHERE InvoiceNo = {oneRandomInvoice}\n",
    "           \"\"\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[]"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# Update one invoice\n",
    "\n",
    "\n",
    "#deltaTable.update(\n",
    "#    condition=(\"InvoiceNo = oneRandomInvoice\"),\n",
    "#    set={\"Quantity\": expr(\"Quantity + 1000\")}\n",
    "#)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "               UPDATE SalesDeltaFormat\n",
    "               SET Quantity = Quantity + 1000\n",
    "               WHERE InvoiceNo = {oneRandomInvoice}\n",
    "           \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------+-------------------+------+--------+---------+----------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------+------------+\n|version|timestamp          |userId|userName|operation|operationParameters                     |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                       |userMetadata|\n+-------+-------------------+------+--------+---------+----------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------+------------+\n|4      |2020-11-11 22:42:23|null  |null    |UPDATE   |[predicate -> (InvoiceNo#3224 = 554207)]|null|null    |null     |3          |null          |false        |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 37483]|null        |\n|3      |2020-11-11 22:40:01|null  |null    |WRITE    |[mode -> Append, partitionBy -> []]     |null|null    |null     |2          |null          |true         |[numFiles -> 1, numOutputBytes -> 2369, numOutputRows -> 1]                            |null        |\n|2      |2020-11-11 22:36:42|null  |null    |WRITE    |[mode -> Append, partitionBy -> []]     |null|null    |null     |1          |null          |true         |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]                     |null        |\n|1      |2020-11-11 22:35:49|null  |null    |WRITE    |[mode -> Append, partitionBy -> []]     |null|null    |null     |0          |null          |true         |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]                     |null        |\n|0      |2020-11-11 22:32:17|null  |null    |WRITE    |[mode -> Overwrite, partitionBy -> []]  |null|null    |null     |null       |null          |false        |[numFiles -> 3, numOutputBytes -> 1083137, numOutputRows -> 99057]                     |null        |\n+-------+-------------------+------+--------+---------+----------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------+------------+\n\n"
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE HISTORY SalesDeltaFormat\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------------------------------------------------------------------+---------+---------+----------------------------------+--------+----------------+---------+----------+--------------+\n|FileName                                                           |InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate     |UnitPrice|CustomerID|Country       |\n+-------------------------------------------------------------------+---------+---------+----------------------------------+--------+----------------+---------+----------+--------------+\n|part-00000-439e822e-6e49-42c2-9c9d-5d35162ba87f-c000.snappy.parquet|554485   |23152    |IVORY SWEETHEART WIRE LETTER RACK |1002    |5/24/2011 13:22 |3.75     |18018     |United Kingdom|\n|part-00001-6f9a3c57-30d1-43e3-b580-0c1e65b85524-c000.snappy.parquet|554485   |2291     |WORLD'S BEST JAM MAKING SET       |1005    |08/13/2011 07:58|1.45     |15358     |France        |\n+-------------------------------------------------------------------+---------+---------+----------------------------------+--------+----------------+---------+----------+--------------+\n\n"
    }
   ],
   "source": [
    "# After Update\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "               SELECT\n",
    "               SUBSTRING(input_file_name(), -67, 67) AS FileName, *\n",
    "               FROM SalesDeltaFormat\n",
    "               WHERE InvoiceNo = {oneRandomInvoice}\n",
    "           \"\"\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "####### HISTORY ########\n+---+-------------------+---------+----------------------------------------+---------------------------------------------------------------------------------------+\n|ver|timestamp          |operation|operationParameters                     |operationMetrics                                                                       |\n+---+-------------------+---------+----------------------------------------+---------------------------------------------------------------------------------------+\n|4  |2020-11-11 22:42:23|UPDATE   |[predicate -> (InvoiceNo#3224 = 554207)]|[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 37483]|\n|3  |2020-11-11 22:40:01|WRITE    |[mode -> Append, partitionBy -> []]     |[numFiles -> 1, numOutputBytes -> 2369, numOutputRows -> 1]                            |\n|2  |2020-11-11 22:36:42|WRITE    |[mode -> Append, partitionBy -> []]     |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]                     |\n|1  |2020-11-11 22:35:49|WRITE    |[mode -> Append, partitionBy -> []]     |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]                     |\n|0  |2020-11-11 22:32:17|WRITE    |[mode -> Overwrite, partitionBy -> []]  |[numFiles -> 3, numOutputBytes -> 1083137, numOutputRows -> 99057]                     |\n+---+-------------------+---------+----------------------------------------+---------------------------------------------------------------------------------------+\n\n"
    }
   ],
   "source": [
    "print(\"####### HISTORY ########\")\n",
    "\n",
    "# Show which datafile was removed.\n",
    "\n",
    "# Observe history of actions taken on a Delta table\n",
    "# spark.sql not supported in 0.7.0 OSS Delta\n",
    "\n",
    "history = deltaTable.history().select('version', 'timestamp', 'operation', 'operationParameters', 'operationMetrics')\\\n",
    "                              .withColumnRenamed('version', 'ver')\n",
    "history.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "279KB    2020-11-11 16:09:29  part-00002-4f72cf15-cb0b-414d-921d-9aa9abb594fb-c000.snappy.parquet\n399KB    2020-11-11 16:09:29  part-00001-28fd7101-7f97-4028-83c5-5dcdd9b20754-c000.snappy.parquet\n379KB    2020-11-11 16:09:29  part-00000-e09d4d65-b48f-48ff-bb11-32ec31c62321-c000.snappy.parquet\n281KB    2020-11-11 16:09:59  part-00002-e86f84af-b8c7-46c1-9339-e4e13871b47f-c000.snappy.parquet\n379KB    2020-11-11 16:09:59  part-00000-6862b1f2-21bf-426b-926c-e8c0d3859485-c000.snappy.parquet\n398KB    2020-11-11 16:09:59  part-00001-baff5b51-e2f0-4120-b7b6-56d621641096-c000.snappy.parquet\n2KB      2020-11-11 16:10:31  part-00000-b783e06e-a2cc-4d2d-ab2f-ba4bbe61552a-c000.snappy.parquet\n2KB      2020-11-11 16:10:53  part-00001-6f9a3c57-30d1-43e3-b580-0c1e65b85524-c000.snappy.parquet\n398KB    2020-11-11 16:10:53  part-00000-439e822e-6e49-42c2-9c9d-5d35162ba87f-c000.snappy.parquet\n\nNumber of file/s: 9 | Total size: 2.6M\n"
    }
   ],
   "source": [
    "files_in_dir(deltaPath, \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2KB      2020-11-11 16:09:30  00000000000000000000.json\n765B     2020-11-11 16:09:59  00000000000000000001.json\n410B     2020-11-11 16:10:31  00000000000000000002.json\n902B     2020-11-11 16:10:53  00000000000000000003.json\n\nNumber of file/s: 4 | Total size: 20K\n"
    }
   ],
   "source": [
    "files_in_dir(deltaLogPath,\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(add=None, commitInfo=Row(isBlindAppend=True, operation='WRITE', operationMetrics=Row(numFiles='1', numOutputBytes='2369', numOutputRows='1'), operationParameters=Row(mode='Append', partitionBy='[]'), readVersion=2, timestamp=1605156001162)),\n Row(add=Row(dataChange=True, modificationTime=1605156001000, path='part-00000-30dbc080-1ec2-4d27-81e4-05e205c846ae-c000.snappy.parquet', size=2369), commitInfo=None)]"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "# Schema details: https://docs.delta.io/latest/delta-utility.html\n",
    "\n",
    "logDF = spark.read.format(\"json\").load(deltaLogPath + \"/00000000000000000003.json\")\n",
    "logDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------------------------------------------------------------------+---------+---------+----------------------------------+--------+----------------+---------+----------+--------------+\n|FileName                                                           |InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate     |UnitPrice|CustomerID|Country       |\n+-------------------------------------------------------------------+---------+---------+----------------------------------+--------+----------------+---------+----------+--------------+\n|part-00000-439e822e-6e49-42c2-9c9d-5d35162ba87f-c000.snappy.parquet|554485   |23152    |IVORY SWEETHEART WIRE LETTER RACK |1002    |5/24/2011 13:22 |3.75     |18018     |United Kingdom|\n|part-00001-6f9a3c57-30d1-43e3-b580-0c1e65b85524-c000.snappy.parquet|554485   |2291     |WORLD'S BEST JAM MAKING SET       |1005    |08/13/2011 07:58|1.45     |15358     |France        |\n+-------------------------------------------------------------------+---------+---------+----------------------------------+--------+----------------+---------+----------+--------------+\n\n"
    }
   ],
   "source": [
    "# Before DML (delete)\n",
    "\n",
    "spark.sql(f\"\"\" SELECT \n",
    "               substring(input_file_name(), -67, 67) as FileName,\n",
    "               * from SalesDeltaFormat \n",
    "               WHERE InvoiceNo = {oneRandomInvoice}\n",
    "           \"\"\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[]"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "# https://github.com/delta-io/delta/blob/master/examples/python/quickstart.py\n",
    "# Delete and invoice (two records)\n",
    "\n",
    "# This results in one new file being created.  One file had just the one record so it does not have to be re-created\n",
    "# Each of the two records were in two different files. One of those files had only one record so it did not have to be re-created.\n",
    "\n",
    "spark.sql(f\"DELETE FROM SalesDeltaFormat WHERE InvoiceNo = {oneRandomInvoice}\")\n",
    "# deltaTable.delete(\n",
    "#    condition=(\"InvoiceNo = {537617}\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------+-------------------+------+--------+---------+----------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------+------------+\n|version|timestamp          |userId|userName|operation|operationParameters                                                               |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                       |userMetadata|\n+-------+-------------------+------+--------+---------+----------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------+------------+\n|5      |2020-11-11 22:44:48|null  |null    |DELETE   |[predicate -> [\"(spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = 554207)\"]]|null|null    |null     |4          |null          |false        |[numRemovedFiles -> 2, numDeletedRows -> 2, numAddedFiles -> 1, numCopiedRows -> 37483]|null        |\n|4      |2020-11-11 22:42:23|null  |null    |UPDATE   |[predicate -> (InvoiceNo#3224 = 554207)]                                          |null|null    |null     |3          |null          |false        |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 37483]|null        |\n|3      |2020-11-11 22:40:01|null  |null    |WRITE    |[mode -> Append, partitionBy -> []]                                               |null|null    |null     |2          |null          |true         |[numFiles -> 1, numOutputBytes -> 2369, numOutputRows -> 1]                            |null        |\n|2      |2020-11-11 22:36:42|null  |null    |WRITE    |[mode -> Append, partitionBy -> []]                                               |null|null    |null     |1          |null          |true         |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]                     |null        |\n|1      |2020-11-11 22:35:49|null  |null    |WRITE    |[mode -> Append, partitionBy -> []]                                               |null|null    |null     |0          |null          |true         |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]                     |null        |\n|0      |2020-11-11 22:32:17|null  |null    |WRITE    |[mode -> Overwrite, partitionBy -> []]                                            |null|null    |null     |null       |null          |false        |[numFiles -> 3, numOutputBytes -> 1083137, numOutputRows -> 99057]                     |null        |\n+-------+-------------------+------+--------+---------+----------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------+------------+\n\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n\n"
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE HISTORY SalesDeltaFormat\").show(truncate = False)\n",
    "spark.sql(f\"SELECT * FROM SalesDeltaFormat WHERE InvoiceNo = {oneRandomInvoice}\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+--------+---------+---------+-----------+--------+-----------+---------+----------+-------+\n|FileName|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n+--------+---------+---------+-----------+--------+-----------+---------+----------+-------+\n+--------+---------+---------+-----------+--------+-----------+---------+----------+-------+\n\n"
    }
   ],
   "source": [
    "# After DML (delete)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "              SELECT \n",
    "              SUBSTRING(input_file_name(), -67, 67) as FileName, *\n",
    "              FROM SalesDeltaFormat \n",
    "              WHERE InvoiceNo = {oneRandomInvoice}\n",
    "          \"\"\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "####### HISTORY ########\n+---+---------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n|ver|operation|operationParameters                                                               |operationMetrics                                                                       |\n+---+---------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n|5  |DELETE   |[predicate -> [\"(spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = 554207)\"]]|[numRemovedFiles -> 2, numDeletedRows -> 2, numAddedFiles -> 1, numCopiedRows -> 37483]|\n|4  |UPDATE   |[predicate -> (InvoiceNo#3224 = 554207)]                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 37483]|\n|3  |WRITE    |[mode -> Append, partitionBy -> []]                                               |[numFiles -> 1, numOutputBytes -> 2369, numOutputRows -> 1]                            |\n|2  |WRITE    |[mode -> Append, partitionBy -> []]                                               |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]                     |\n|1  |WRITE    |[mode -> Append, partitionBy -> []]                                               |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]                     |\n|0  |WRITE    |[mode -> Overwrite, partitionBy -> []]                                            |[numFiles -> 3, numOutputBytes -> 1083137, numOutputRows -> 99057]                     |\n+---+---------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n\n"
    }
   ],
   "source": [
    "print(\"####### HISTORY ########\")\n",
    "\n",
    "# Observe history of actions taken on a Delta table\n",
    "history = deltaTable.history().select('version','operation', 'operationParameters', \\\n",
    "                                      'operationMetrics') \\\n",
    "                              .withColumnRenamed(\"version\", \"ver\")\n",
    "\n",
    "history.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "279KB    2020-11-11 16:09:29  part-00002-4f72cf15-cb0b-414d-921d-9aa9abb594fb-c000.snappy.parquet\n399KB    2020-11-11 16:09:29  part-00001-28fd7101-7f97-4028-83c5-5dcdd9b20754-c000.snappy.parquet\n379KB    2020-11-11 16:09:29  part-00000-e09d4d65-b48f-48ff-bb11-32ec31c62321-c000.snappy.parquet\n281KB    2020-11-11 16:09:59  part-00002-e86f84af-b8c7-46c1-9339-e4e13871b47f-c000.snappy.parquet\n379KB    2020-11-11 16:09:59  part-00000-6862b1f2-21bf-426b-926c-e8c0d3859485-c000.snappy.parquet\n398KB    2020-11-11 16:09:59  part-00001-baff5b51-e2f0-4120-b7b6-56d621641096-c000.snappy.parquet\n2KB      2020-11-11 16:10:31  part-00000-b783e06e-a2cc-4d2d-ab2f-ba4bbe61552a-c000.snappy.parquet\n2KB      2020-11-11 16:10:53  part-00001-6f9a3c57-30d1-43e3-b580-0c1e65b85524-c000.snappy.parquet\n398KB    2020-11-11 16:10:53  part-00000-439e822e-6e49-42c2-9c9d-5d35162ba87f-c000.snappy.parquet\n399KB    2020-11-11 16:11:17  part-00000-7ce26bc6-9b27-4622-a970-21a895241fca-c000.snappy.parquet\n\nNumber of file/s: 10 | Total size: 3.0M\n"
    }
   ],
   "source": [
    "files_in_dir(deltaPath, \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2KB      2020-11-11 16:09:30  00000000000000000000.json\n765B     2020-11-11 16:09:59  00000000000000000001.json\n410B     2020-11-11 16:10:31  00000000000000000002.json\n902B     2020-11-11 16:10:53  00000000000000000003.json\n775B     2020-11-11 16:11:17  00000000000000000004.json\n\nNumber of file/s: 5 | Total size: 24K\n"
    }
   ],
   "source": [
    "files_in_dir(deltaLogPath,\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(add=None, commitInfo=Row(isBlindAppend=False, operation='DELETE', operationMetrics=Row(numAddedFiles='1', numCopiedRows='37483', numDeletedRows='2', numRemovedFiles='2'), operationParameters=Row(predicate='[\"(spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = 554485)\"]'), readVersion=3, timestamp=1605132677391), remove=None),\n Row(add=None, commitInfo=None, remove=Row(dataChange=True, deletionTimestamp=1605132677389, path='part-00000-439e822e-6e49-42c2-9c9d-5d35162ba87f-c000.snappy.parquet')),\n Row(add=None, commitInfo=None, remove=Row(dataChange=True, deletionTimestamp=1605132677389, path='part-00001-6f9a3c57-30d1-43e3-b580-0c1e65b85524-c000.snappy.parquet')),\n Row(add=Row(dataChange=True, modificationTime=1605132677000, path='part-00000-7ce26bc6-9b27-4622-a970-21a895241fca-c000.snappy.parquet', size=408279), commitInfo=None, remove=None)]"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "logDF = spark.read.format(\"json\").load(deltaLogPath + \"/00000000000000000004.json\")\n",
    "#dfLog.printSchema()\n",
    "logDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomy update 5 random invoices to force a checkpoint\n",
    "\n",
    "count = 0\n",
    "anInvoice = retailSalesData2.select(\"InvoiceNo\").orderBy(rand()).limit(1).collect()[0][0]\n",
    "\n",
    "while (count <= 5):\n",
    "  deltaTable.update(\n",
    "    condition=(f\"InvoiceNo = {anInvoice}\"),\n",
    "    set={\"Quantity\": expr(\"Quantity + 100\")})\n",
    "\n",
    "  count = count + 1\n",
    "  anInvoice = retailSalesData2.select(\"InvoiceNo\").orderBy(rand()).limit(1).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2KB      2020-11-11 22:32:17  00000000000000000000.json\n765B     2020-11-11 22:35:49  00000000000000000001.json\n765B     2020-11-11 22:36:42  00000000000000000002.json\n410B     2020-11-11 22:40:01  00000000000000000003.json\n902B     2020-11-11 22:42:23  00000000000000000004.json\n775B     2020-11-11 22:44:48  00000000000000000005.json\n1KB      2020-11-11 22:49:39  00000000000000000006.json\n1KB      2020-11-11 22:49:40  00000000000000000007.json\n1KB      2020-11-11 22:49:42  00000000000000000008.json\n1KB      2020-11-11 22:49:44  00000000000000000009.json\n1KB      2020-11-11 22:49:45  00000000000000000010.json\n18KB     2020-11-11 22:49:46  00000000000000000010.checkpoint.parquet\n1KB      2020-11-11 22:49:47  00000000000000000011.json\n\nNumber of file/s: 13 | Total size: 80K\n"
    }
   ],
   "source": [
    "files_in_dir(deltaLogPath,\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "b38ca5-7f0a-472e-b176-f72af7be81d8-c000.snappy.parquet, 1605156288183, false]|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|null                                                                                                     |[part-00002-4d0a18a3-0d19-4555-85c9-2cb8e4185175-c000.snappy.parquet, 1605156582345, false]|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|null                                                                                                     |[part-00002-4fb0ac34-50da-42f9-9e13-5dec4d5193fa-c000.snappy.parquet, 1605156585478, false]|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|[part-00000-89405477-caf4-41fe-b360-a49c3941d592-c000.snappy.parquet, [], 388304, 1605156584000, false,,]|null                                                                                       |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|null                                                                                                     |[part-00001-681c6373-595c-4d65-849d-b8197bfc2106-c000.snappy.parquet, 1605156583946, false]|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|null                                                                                                     |[part-00002-172a1fcd-26b8-4489-94fd-e243d09d4d6f-c000.snappy.parquet, 1605156585478, false]|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|null                                                                                                     |[part-00000-05e91c6c-367c-4ce0-9b58-c12c2fe68335-c000.snappy.parquet, 1605156582345, false]|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|null                                                                                                     |[part-00002-1552dc81-b629-4a72-a68d-859a35fae432-c000.snappy.parquet, 1605156580747, false]|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|null                                                                                                     |[part-00000-30dbc080-1ec2-4d27-81e4-05e205c846ae-c000.snappy.parquet, 1605156143226, false]|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|null                                                                                                     |[part-00001-9b8da00e-db5b-4b3a-8ece-a4243cee3aa1-c000.snappy.parquet, 1605156582345, false]|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|null                                                                                                     |[part-00001-5b6001f7-4bef-4520-b0b3-6b4032a6f01f-c000.snappy.parquet, 1605156580747, false]|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|null                                                                                                     |[part-00000-5041cc9f-fb3a-454b-8766-0e187f70fe73-c000.snappy.parquet, 1605156580747, false]|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|[part-00000-397db2d8-e173-44c4-a3ba-5cc948221449-c000.snappy.parquet, [], 287510, 1605156585000, false,,]|null                                                                                       |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|[part-00000-8badf00f-a3ca-42ae-8047-48ae2939f7b1-c000.snappy.parquet, [], 408423, 1605156288000, false,,]|null                                                                                       |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|[part-00001-89903254-8f47-419c-8590-5b4d5a3879c7-c000.snappy.parquet, [], 388197, 1605156584000, false,,]|null                                                                                       |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|[part-00001-d1c13968-1579-45f7-ab23-7ff186144cab-c000.snappy.parquet, [], 287510, 1605156585000, false,,]|null                                                                                       |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|[part-00002-5990853e-d9c7-4282-b44d-69bc441078fa-c000.snappy.parquet, [], 388197, 1605156584000, false,,]|null                                                                                       |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|null                                                                                                     |null                                                                                       |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |[1, 2]  |null      |\n|null|null                                                                                                     |null                                                                                       |[9276dfd5-1dac-4ac2-8786-ec0281e9a0f7,,, [parquet, []], {\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNo\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StockCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Description\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Quantity\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"InvoiceDate\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"UnitPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerID\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Country\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}, [], [], 1605155535682]|null    |null      |\n|null|[part-00001-2dc9751d-69e8-40a2-add5-2b0bf22e6313-c000.snappy.parquet, [], 407069, 1605155749000, false,,]|null                                                                                       |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|null                                                                                                     |[part-00002-e4e27572-4813-4fb0-8aa4-b3615113ec7e-c000.snappy.parquet, 1605156583946, false]|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n|null|null                                                                                                     |[part-00001-204be439-2710-430f-9555-28489396e9e7-c000.snappy.parquet, 1605156288183, false]|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |null    |null      |\n+----+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+----------+\n\n"
    }
   ],
   "source": [
    "checkPointDF = spark.read.format(\"parquet\").load(deltaLogPath + \"/00000000000000000010.checkpoint.parquet\")\n",
    "checkPointDF.show(100, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkPointFile10 = (    \n",
    "    checkPointDF.select(col(\"add.path\").alias(\"FileAdded\"),\n",
    "                        col(\"add.modificationTime\").alias(\"DateAdded\"),\n",
    "                        col(\"remove.path\").alias(\"FileDeleted\"),\n",
    "                        col(\"remove.deletionTimestamp\").alias(\"DateDeleted\"))\n",
    "                .orderBy([\"DateAdded\", \"DateDeleted\"], ascending = [True, False])\n",
    ")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS tbl_checkpointfile\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS tbl_checkpointfile (Action string, filename string, ActionDate Long)\")# Crea la tabla tbl_checkpointfile\n",
    "# dentro del folder 'DATABASE', ya que no se especificó 'LOCATION' y en este caso se crea una tabla vacia\n",
    "\n",
    "checkPointFile10.createOrReplaceTempView(\"vw_checkpointfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------------------------------------------------------------------+-------------+-------------------------------------------------------------------+-------------+\n|FileAdded                                                          |DateAdded    |FileDeleted                                                        |DateDeleted  |\n+-------------------------------------------------------------------+-------------+-------------------------------------------------------------------+-------------+\n|null                                                               |null         |part-00001-9daea6bb-7c90-4378-aa1e-4b442b914e27-c000.snappy.parquet|1605132699095|\n|null                                                               |null         |part-00000-9b9ec761-6e6b-4aad-a254-3469ccac8e20-c000.snappy.parquet|1605132699095|\n|null                                                               |null         |part-00000-4129a15a-895f-4cbf-b83f-d861256264ef-c000.snappy.parquet|1605132697485|\n|null                                                               |null         |part-00001-c7b6b59c-c3a7-4346-a884-c6bb6c7cbaae-c000.snappy.parquet|1605132697485|\n|null                                                               |null         |part-00000-7c28ebe1-33a8-4602-aa24-25955c2e8229-c000.snappy.parquet|1605132695964|\n|null                                                               |null         |part-00001-1cd69ff7-313f-46e7-aea1-76b7165ecb83-c000.snappy.parquet|1605132695964|\n|null                                                               |null         |part-00000-c0c1851d-e81b-4e5d-9468-cde75ddfafd5-c000.snappy.parquet|1605132694433|\n|null                                                               |null         |part-00001-f6e11f04-e9bc-45ff-9593-e9c3d553a0d2-c000.snappy.parquet|1605132694433|\n|null                                                               |null         |part-00000-e09d4d65-b48f-48ff-bb11-32ec31c62321-c000.snappy.parquet|1605132692892|\n|null                                                               |null         |part-00000-6862b1f2-21bf-426b-926c-e8c0d3859485-c000.snappy.parquet|1605132692892|\n|null                                                               |null         |part-00000-7ce26bc6-9b27-4622-a970-21a895241fca-c000.snappy.parquet|1605132691238|\n|null                                                               |null         |part-00001-baff5b51-e2f0-4120-b7b6-56d621641096-c000.snappy.parquet|1605132691238|\n|null                                                               |null         |part-00001-6f9a3c57-30d1-43e3-b580-0c1e65b85524-c000.snappy.parquet|1605132677389|\n|null                                                               |null         |part-00000-439e822e-6e49-42c2-9c9d-5d35162ba87f-c000.snappy.parquet|1605132677389|\n|null                                                               |null         |part-00000-b783e06e-a2cc-4d2d-ab2f-ba4bbe61552a-c000.snappy.parquet|1605132653696|\n|null                                                               |null         |part-00001-28fd7101-7f97-4028-83c5-5dcdd9b20754-c000.snappy.parquet|1605132653696|\n|null                                                               |null         |null                                                               |null         |\n|null                                                               |null         |null                                                               |null         |\n|part-00002-4f72cf15-cb0b-414d-921d-9aa9abb594fb-c000.snappy.parquet|1605132569000|null                                                               |null         |\n|part-00002-e86f84af-b8c7-46c1-9339-e4e13871b47f-c000.snappy.parquet|1605132599000|null                                                               |null         |\n|part-00001-afe6cb6d-e5d7-4d47-809a-3246629e1034-c000.snappy.parquet|1605132697000|null                                                               |null         |\n|part-00000-4df93499-b49d-44e2-9542-4a28bef76c5a-c000.snappy.parquet|1605132697000|null                                                               |null         |\n|part-00001-bc0b63d4-4087-43dd-a3c1-231008e9a9a1-c000.snappy.parquet|1605132699000|null                                                               |null         |\n|part-00000-59a5222a-b275-48a5-924e-7cd1a3f35c04-c000.snappy.parquet|1605132699000|null                                                               |null         |\n+-------------------------------------------------------------------+-------------+-------------------------------------------------------------------+-------------+\n\n"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM vw_checkpointfile LIMIT 100\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[]"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "              INSERT INTO tbl_checkpointfile\n",
    "              SELECT \"Add\", FileAdded, DateAdded\n",
    "              FROM vw_checkpointfile\n",
    "              WHERE FileAdded IS NOT NULL\n",
    "          \"\"\")\n",
    "# Aquí es donde se añaden los datos a la tabla 'tbl_checkpointfile' \n",
    "spark.sql(\"\"\"\n",
    "              INSERT INTO tbl_checkpointfile\n",
    "              SELECT \"Delete\", FileDeleted, DateDeleted\n",
    "              FROM vw_checkpointfile\n",
    "              WHERE FileDeleted IS NOT NULL\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+-------------------------------------------------------------------+-------------------+\n|Action|File Name                                                          |ActionDate         |\n+------+-------------------------------------------------------------------+-------------------+\n|Add   |part-00002-4f72cf15-cb0b-414d-921d-9aa9abb594fb-c000.snappy.parquet|2020-11-11 16:09:29|\n|Add   |part-00002-e86f84af-b8c7-46c1-9339-e4e13871b47f-c000.snappy.parquet|2020-11-11 16:09:59|\n|Delete|part-00000-b783e06e-a2cc-4d2d-ab2f-ba4bbe61552a-c000.snappy.parquet|2020-11-11 16:10:53|\n|Delete|part-00001-28fd7101-7f97-4028-83c5-5dcdd9b20754-c000.snappy.parquet|2020-11-11 16:10:53|\n|Delete|part-00001-6f9a3c57-30d1-43e3-b580-0c1e65b85524-c000.snappy.parquet|2020-11-11 16:11:17|\n|Delete|part-00000-439e822e-6e49-42c2-9c9d-5d35162ba87f-c000.snappy.parquet|2020-11-11 16:11:17|\n|Delete|part-00001-baff5b51-e2f0-4120-b7b6-56d621641096-c000.snappy.parquet|2020-11-11 16:11:31|\n|Delete|part-00000-7ce26bc6-9b27-4622-a970-21a895241fca-c000.snappy.parquet|2020-11-11 16:11:31|\n|Delete|part-00000-e09d4d65-b48f-48ff-bb11-32ec31c62321-c000.snappy.parquet|2020-11-11 16:11:32|\n|Delete|part-00000-6862b1f2-21bf-426b-926c-e8c0d3859485-c000.snappy.parquet|2020-11-11 16:11:32|\n|Delete|part-00001-f6e11f04-e9bc-45ff-9593-e9c3d553a0d2-c000.snappy.parquet|2020-11-11 16:11:34|\n|Delete|part-00000-c0c1851d-e81b-4e5d-9468-cde75ddfafd5-c000.snappy.parquet|2020-11-11 16:11:34|\n|Delete|part-00000-7c28ebe1-33a8-4602-aa24-25955c2e8229-c000.snappy.parquet|2020-11-11 16:11:35|\n|Delete|part-00001-1cd69ff7-313f-46e7-aea1-76b7165ecb83-c000.snappy.parquet|2020-11-11 16:11:35|\n|Add   |part-00000-4df93499-b49d-44e2-9542-4a28bef76c5a-c000.snappy.parquet|2020-11-11 16:11:37|\n|Add   |part-00001-afe6cb6d-e5d7-4d47-809a-3246629e1034-c000.snappy.parquet|2020-11-11 16:11:37|\n|Delete|part-00000-4129a15a-895f-4cbf-b83f-d861256264ef-c000.snappy.parquet|2020-11-11 16:11:37|\n|Delete|part-00001-c7b6b59c-c3a7-4346-a884-c6bb6c7cbaae-c000.snappy.parquet|2020-11-11 16:11:37|\n|Add   |part-00001-bc0b63d4-4087-43dd-a3c1-231008e9a9a1-c000.snappy.parquet|2020-11-11 16:11:39|\n|Delete|part-00001-9daea6bb-7c90-4378-aa1e-4b442b914e27-c000.snappy.parquet|2020-11-11 16:11:39|\n|Add   |part-00000-59a5222a-b275-48a5-924e-7cd1a3f35c04-c000.snappy.parquet|2020-11-11 16:11:39|\n|Delete|part-00000-9b9ec761-6e6b-4aad-a254-3469ccac8e20-c000.snappy.parquet|2020-11-11 16:11:39|\n+------+-------------------------------------------------------------------+-------------------+\n\n"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "              SELECT Action, filename AS `File Name`,\n",
    "              from_unixtime(actiondate/1e3) AS `ActionDate`\n",
    "              FROM tbl_checkpointfile\n",
    "              ORDER BY ActionDate ASC\n",
    "          \"\"\").show(200, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[]"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "# Let's add some data to our table by doing a merge\n",
    "# Do this to show how json logs pick up after checkpoint\n",
    "\n",
    "# Create a tiny dataframe to use with merge\n",
    "mergeSalesData= cleanSalesDataDF.sample(withReplacement=False, fraction=.0001, seed=13)\n",
    "mergeSalesData.createOrReplaceTempView(\"vw_mergeSalesData\")\n",
    "\n",
    "# User-defined commit metadata\n",
    "spark.sql(f\"\"\"\n",
    "               SET spark.databricks.delta.commitInfo.userMetadata = 11-11-2020 Data Merge Message;\n",
    "          \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          MERGE INTO SalesDeltaFormat\n",
    "          USING vw_mergeSalesData\n",
    "          ON SalesDeltaFormat.StockCode = vw_mergeSalesData.StockCode\n",
    "           AND SalesDeltaFormat.InvoiceNo = vw_mergeSalesData.InvoiceNo\n",
    "          WHEN MATCHED THEN\n",
    "            UPDATE SET *\n",
    "          WHEN NOT MATCHED\n",
    "            THEN INSERT *\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------+-------------------+------+--------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------+\n|version|timestamp          |userId|userName|operation|operationParameters                                                                                                                                                                               |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                       |userMetadata                  |\n+-------+-------------------+------+--------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------+\n|12     |2020-11-11 22:54:10|null  |null    |MERGE    |[predicate -> ((spark_catalog.deltademo.SalesDeltaFormat.`StockCode` = vw_mergesalesdata.`StockCode`) AND (spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = vw_mergesalesdata.`InvoiceNo`))]|null|null    |null     |11         |null          |false        |[numTargetRowsCopied -> 297942, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 4, numTargetRowsInserted -> 23, numTargetRowsUpdated -> 40, numOutputRows -> 298005, numSourceRows -> 44, numTargetFilesRemoved -> 9]|11-11-2020 Data Merge Message;|\n|11     |2020-11-11 22:49:47|null  |null    |UPDATE   |[predicate -> (InvoiceNo#1222 = 561915)]                                                                                                                                                          |null|null    |null     |10         |null          |false        |[numRemovedFiles -> 3, numAddedFiles -> 3, numUpdatedRows -> 32, numCopiedRows -> 112421]                                                                                                                              |null                          |\n|10     |2020-11-11 22:49:45|null  |null    |UPDATE   |[predicate -> (InvoiceNo#1222 = 574063)]                                                                                                                                                          |null|null    |null     |9          |null          |false        |[numRemovedFiles -> 3, numAddedFiles -> 3, numUpdatedRows -> 62, numCopiedRows -> 82370]                                                                                                                               |null                          |\n|9      |2020-11-11 22:49:44|null  |null    |UPDATE   |[predicate -> (InvoiceNo#1222 = 548806)]                                                                                                                                                          |null|null    |null     |8          |null          |false        |[numRemovedFiles -> 3, numAddedFiles -> 3, numUpdatedRows -> 10, numCopiedRows -> 103087]                                                                                                                              |null                          |\n|8      |2020-11-11 22:49:42|null  |null    |UPDATE   |[predicate -> (InvoiceNo#1222 = 544943)]                                                                                                                                                          |null|null    |null     |7          |null          |false        |[numRemovedFiles -> 3, numAddedFiles -> 3, numUpdatedRows -> 25, numCopiedRows -> 103072]                                                                                                                              |null                          |\n|7      |2020-11-11 22:49:40|null  |null    |UPDATE   |[predicate -> (InvoiceNo#1222 = 548497)]                                                                                                                                                          |null|null    |null     |6          |null          |false        |[numRemovedFiles -> 3, numAddedFiles -> 3, numUpdatedRows -> 42, numCopiedRows -> 103055]                                                                                                                              |null                          |\n|6      |2020-11-11 22:49:39|null  |null    |UPDATE   |[predicate -> (InvoiceNo#1222 = 551471)]                                                                                                                                                          |null|null    |null     |5          |null          |false        |[numRemovedFiles -> 3, numAddedFiles -> 3, numUpdatedRows -> 61, numCopiedRows -> 103036]                                                                                                                              |null                          |\n|5      |2020-11-11 22:44:48|null  |null    |DELETE   |[predicate -> [\"(spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = 554207)\"]]                                                                                                                |null|null    |null     |4          |null          |false        |[numRemovedFiles -> 2, numDeletedRows -> 2, numAddedFiles -> 1, numCopiedRows -> 37483]                                                                                                                                |null                          |\n|4      |2020-11-11 22:42:23|null  |null    |UPDATE   |[predicate -> (InvoiceNo#3224 = 554207)]                                                                                                                                                          |null|null    |null     |3          |null          |false        |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 37483]                                                                                                                                |null                          |\n|3      |2020-11-11 22:40:01|null  |null    |WRITE    |[mode -> Append, partitionBy -> []]                                                                                                                                                               |null|null    |null     |2          |null          |true         |[numFiles -> 1, numOutputBytes -> 2369, numOutputRows -> 1]                                                                                                                                                            |null                          |\n|2      |2020-11-11 22:36:42|null  |null    |WRITE    |[mode -> Append, partitionBy -> []]                                                                                                                                                               |null|null    |null     |1          |null          |true         |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]                                                                                                                                                     |null                          |\n|1      |2020-11-11 22:35:49|null  |null    |WRITE    |[mode -> Append, partitionBy -> []]                                                                                                                                                               |null|null    |null     |0          |null          |true         |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]                                                                                                                                                     |null                          |\n|0      |2020-11-11 22:32:17|null  |null    |WRITE    |[mode -> Overwrite, partitionBy -> []]                                                                                                                                                            |null|null    |null     |null       |null          |false        |[numFiles -> 3, numOutputBytes -> 1083137, numOutputRows -> 99057]                                                                                                                                                     |null                          |\n+-------+-------------------+------+--------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------+\n\n"
    }
   ],
   "source": [
    "spark.sql(\"describe history SalesDeltaFormat\").show(truncate = False)# para ver el mensaje de 'userMetadata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "####### HISTORY ########\n+---+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|ver|operation|operationParameters                                                                                                                                                                               |operationMetrics                                                                                                                                                                                                       |\n+---+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|12 |MERGE    |[predicate -> ((spark_catalog.deltademo.SalesDeltaFormat.`StockCode` = vw_mergesalesdata.`StockCode`) AND (spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = vw_mergesalesdata.`InvoiceNo`))]|[numTargetRowsCopied -> 297942, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 4, numTargetRowsInserted -> 23, numTargetRowsUpdated -> 40, numOutputRows -> 298005, numSourceRows -> 44, numTargetFilesRemoved -> 9]|\n|11 |UPDATE   |[predicate -> (InvoiceNo#1222 = 561915)]                                                                                                                                                          |[numRemovedFiles -> 3, numAddedFiles -> 3, numUpdatedRows -> 32, numCopiedRows -> 112421]                                                                                                                              |\n|10 |UPDATE   |[predicate -> (InvoiceNo#1222 = 574063)]                                                                                                                                                          |[numRemovedFiles -> 3, numAddedFiles -> 3, numUpdatedRows -> 62, numCopiedRows -> 82370]                                                                                                                               |\n|9  |UPDATE   |[predicate -> (InvoiceNo#1222 = 548806)]                                                                                                                                                          |[numRemovedFiles -> 3, numAddedFiles -> 3, numUpdatedRows -> 10, numCopiedRows -> 103087]                                                                                                                              |\n|8  |UPDATE   |[predicate -> (InvoiceNo#1222 = 544943)]                                                                                                                                                          |[numRemovedFiles -> 3, numAddedFiles -> 3, numUpdatedRows -> 25, numCopiedRows -> 103072]                                                                                                                              |\n|7  |UPDATE   |[predicate -> (InvoiceNo#1222 = 548497)]                                                                                                                                                          |[numRemovedFiles -> 3, numAddedFiles -> 3, numUpdatedRows -> 42, numCopiedRows -> 103055]                                                                                                                              |\n|6  |UPDATE   |[predicate -> (InvoiceNo#1222 = 551471)]                                                                                                                                                          |[numRemovedFiles -> 3, numAddedFiles -> 3, numUpdatedRows -> 61, numCopiedRows -> 103036]                                                                                                                              |\n|5  |DELETE   |[predicate -> [\"(spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = 554207)\"]]                                                                                                                |[numRemovedFiles -> 2, numDeletedRows -> 2, numAddedFiles -> 1, numCopiedRows -> 37483]                                                                                                                                |\n|4  |UPDATE   |[predicate -> (InvoiceNo#3224 = 554207)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 37483]                                                                                                                                |\n|3  |WRITE    |[mode -> Append, partitionBy -> []]                                                                                                                                                               |[numFiles -> 1, numOutputBytes -> 2369, numOutputRows -> 1]                                                                                                                                                            |\n|2  |WRITE    |[mode -> Append, partitionBy -> []]                                                                                                                                                               |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]                                                                                                                                                     |\n|1  |WRITE    |[mode -> Append, partitionBy -> []]                                                                                                                                                               |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]                                                                                                                                                     |\n|0  |WRITE    |[mode -> Overwrite, partitionBy -> []]                                                                                                                                                            |[numFiles -> 3, numOutputBytes -> 1083137, numOutputRows -> 99057]                                                                                                                                                     |\n+---+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"
    }
   ],
   "source": [
    "print(\"####### HISTORY ########\")\n",
    "\n",
    "# Observe history of actions taken on a Delta table\n",
    "history = deltaTable.history().select('version','operation', 'operationParameters', \\\n",
    "                                      'operationMetrics') \\\n",
    "                              .withColumnRenamed(\"version\", \"ver\")\n",
    "\n",
    "history.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2KB      2020-11-11 16:09:30  00000000000000000000.json\n765B     2020-11-11 16:09:59  00000000000000000001.json\n410B     2020-11-11 16:10:31  00000000000000000002.json\n902B     2020-11-11 16:10:53  00000000000000000003.json\n775B     2020-11-11 16:11:17  00000000000000000004.json\n904B     2020-11-11 16:11:31  00000000000000000005.json\n905B     2020-11-11 16:11:33  00000000000000000006.json\n905B     2020-11-11 16:11:34  00000000000000000007.json\n905B     2020-11-11 16:11:36  00000000000000000008.json\n905B     2020-11-11 16:11:37  00000000000000000009.json\n905B     2020-11-11 16:11:39  00000000000000000010.json\n17KB     2020-11-11 16:11:40  00000000000000000010.checkpoint.parquet\n2KB      2020-11-11 16:12:08  00000000000000000011.json\n\nNumber of file/s: 13 | Total size: 80K\n"
    }
   ],
   "source": [
    "files_in_dir(deltaLogPath,\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are ['26'] files.\n"
    }
   ],
   "source": [
    "# Count files in deltaPath\n",
    "\n",
    "numFiles = ! ls $deltaPath/*parquet | wc -l\n",
    "print(f\"There are {numFiles} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+-----------+\n|ACTION|CountAction|\n+------+-----------+\n|Add   |6          |\n|Delete|16         |\n|null  |22         |\n+------+-----------+\n\n"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "              SELECT Action, COUNT(Action) AS `CountAction`\n",
    "              FROM tbl_checkpointfile\n",
    "              GROUP BY ACTION WITH ROLLUP\n",
    "          \"\"\").show(200, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Lake File Compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an artificial \"small file\" problem\n",
    "\n",
    "(spark.read.format(\"delta\").load(deltaPath).repartition(1000).write.option(\"dataChange\", True)\\\n",
    "      .format(\"delta\").mode(\"overwrite\").save(deltaPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are ['1026'] files.\n"
    }
   ],
   "source": [
    "# Count files in deltaPath\n",
    "\n",
    "numFiles = ! ls $deltaPath/*parquet | wc -l\n",
    "print(f\"There are {numFiles} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Row Count => 4248\n\nCPU times: user 1.51 ms, sys: 826 µs, total: 2.34 ms\nWall time: 939 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# spark.sql(\"select * from SalesDeltaFormat limit 2\").show()\n",
    "\n",
    "rowCount = spark.sql(\"\"\" SELECT CustomerID, count(Country) AS num_countries\n",
    "                         FROM SalesDeltaFormat\n",
    "                         GROUP BY CustomerID \n",
    "                     \"\"\").count()\n",
    "\n",
    "print(f\"Row Count => {rowCount}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact 1000 files to 4\n",
    "\n",
    "(spark.read\n",
    ".format(\"delta\")\n",
    ".load(deltaPath)\n",
    ".repartition(4)\n",
    ".write\n",
    ".option(\"dataChange\", False)\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".save(deltaPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are ['1030'] files.\n"
    }
   ],
   "source": [
    "# Count files in deltaPath\n",
    "\n",
    "numFiles = ! ls $deltaPath/*parquet | wc -l\n",
    "print(f\"There are {numFiles} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Row Count => 4248\nCPU times: user 1.04 ms, sys: 567 µs, total: 1.61 ms\nWall time: 239 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# spark.sql(\"select * from SalesDeltaFormat limit 2\").show()\n",
    "rowCount = spark.sql(\"\"\" select CustomerID, count(Country) as num_countries\n",
    "                         from SalesDeltaFormat\n",
    "                        group by CustomerID \"\"\").count()\n",
    "\n",
    "print(f\"Row Count => {rowCount}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Delta Time Travel Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Row Count: 198542 as of table version 13\n\n"
    }
   ],
   "source": [
    "# Time Travel Queries\n",
    "\n",
    "#POO currentVersion = deltaTable.history(1).select(\"version\").collect()[0][0]\n",
    "# Determine latest version of the Delta table\n",
    "currentVersion = spark.sql(\"DESCRIBE HISTORY SalesDeltaFormat LIMIT 1\").collect()[0][0]\n",
    "\n",
    "# Query table as of the current version to attain row count\n",
    "currentRowCount = spark.read.format(\"delta\").option(\"versionAsOf\", currentVersion).load(deltaPath).count()\n",
    "\n",
    "print(f\"Row Count: {currentRowCount} as of table version {currentVersion}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "####### HISTORY ########\n+---+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|ver|operation|operationParameters                                                                                                                                                                               |operationMetrics                                                                                                                                                                                                       |\n+---+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|13 |WRITE    |[mode -> Overwrite, partitionBy -> []]                                                                                                                                                            |[numFiles -> 4, numOutputBytes -> 2968960, numOutputRows -> 198542]                                                                                                                                                    |\n|12 |WRITE    |[mode -> Overwrite, partitionBy -> []]                                                                                                                                                            |[numFiles -> 1000, numOutputBytes -> 11372554, numOutputRows -> 198542]                                                                                                                                                |\n|11 |MERGE    |[predicate -> ((spark_catalog.deltademo.SalesDeltaFormat.`StockCode` = vw_mergesalesdata.`StockCode`) AND (spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = vw_mergesalesdata.`InvoiceNo`))]|[numTargetRowsCopied -> 198493, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 4, numTargetRowsInserted -> 23, numTargetRowsUpdated -> 26, numOutputRows -> 198542, numSourceRows -> 44, numTargetFilesRemoved -> 6]|\n|10 |UPDATE   |[predicate -> (InvoiceNo#1171 = 561648)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 15, numCopiedRows -> 74953]                                                                                                                               |\n|9  |UPDATE   |[predicate -> (InvoiceNo#1171 = 536796)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 69, numCopiedRows -> 68563]                                                                                                                               |\n|8  |UPDATE   |[predicate -> (InvoiceNo#1171 = 557628)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 19, numCopiedRows -> 74949]                                                                                                                               |\n|7  |UPDATE   |[predicate -> (InvoiceNo#1171 = 569718)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 17, numCopiedRows -> 74951]                                                                                                                               |\n|6  |UPDATE   |[predicate -> (InvoiceNo#1171 = 537193)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 21, numCopiedRows -> 68611]                                                                                                                               |\n|5  |UPDATE   |[predicate -> (InvoiceNo#1171 = 569705)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 6, numCopiedRows -> 74962]                                                                                                                                |\n|4  |DELETE   |[predicate -> [\"(spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = 554485)\"]]                                                                                                                |[numRemovedFiles -> 2, numDeletedRows -> 2, numAddedFiles -> 1, numCopiedRows -> 37483]                                                                                                                                |\n|3  |UPDATE   |[predicate -> (InvoiceNo#2697 = 554485)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 37483]                                                                                                                                |\n|2  |WRITE    |[mode -> Append, partitionBy -> []]                                                                                                                                                               |[numFiles -> 1, numOutputBytes -> 2369, numOutputRows -> 1]                                                                                                                                                            |\n|1  |WRITE    |[mode -> Append, partitionBy -> []]                                                                                                                                                               |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]                                                                                                                                                     |\n|0  |WRITE    |[mode -> Overwrite, partitionBy -> []]                                                                                                                                                            |[numFiles -> 3, numOutputBytes -> 1083137, numOutputRows -> 99057]                                                                                                                                                     |\n+---+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"
    }
   ],
   "source": [
    "print(\"####### HISTORY ########\")\n",
    "\n",
    "# Observe history of actions taken on a Delta table\n",
    "history = deltaTable.history().select('version','operation', 'operationParameters', \\\n",
    "                                      'operationMetrics') \\\n",
    "                              .withColumnRenamed(\"version\", \"ver\")\n",
    "\n",
    "history.show(100, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 99485 more rows in version [13] than version [0] of the table.\n"
    }
   ],
   "source": [
    "# Determine difference in record count between the current version and the original version of the table.\n",
    "\n",
    "origRowCount = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(deltaPath).count()\n",
    "print(f\"There are {currentRowCount-origRowCount} more rows in version [{currentVersion}] than version [0] of the table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roll back current table to version 0 (original).\n",
    "(\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .option(\"versionAsOf\",0)\n",
    "    .load(deltaPath)\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(deltaPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "# Current version should have same record count as version 0.\n",
    "\n",
    "currentVersion = spark.sql(\"DESCRIBE HISTORY SalesDeltaFormat LIMIT 1\").collect()[0][0]\n",
    "# If equal it will return \"true\"\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", currentVersion).load(deltaPath).count() == spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(deltaPath).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "####### HISTORY ########\n+---+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|ver|operation|operationParameters                                                                                                                                                                               |operationMetrics                                                                                                                                                                                                       |\n+---+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|14 |WRITE    |[mode -> Overwrite, partitionBy -> []]                                                                                                                                                            |[numFiles -> 3, numOutputBytes -> 1083137, numOutputRows -> 99057]                                                                                                                                                     |\n|13 |WRITE    |[mode -> Overwrite, partitionBy -> []]                                                                                                                                                            |[numFiles -> 4, numOutputBytes -> 2968960, numOutputRows -> 198542]                                                                                                                                                    |\n|12 |WRITE    |[mode -> Overwrite, partitionBy -> []]                                                                                                                                                            |[numFiles -> 1000, numOutputBytes -> 11372554, numOutputRows -> 198542]                                                                                                                                                |\n|11 |MERGE    |[predicate -> ((spark_catalog.deltademo.SalesDeltaFormat.`StockCode` = vw_mergesalesdata.`StockCode`) AND (spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = vw_mergesalesdata.`InvoiceNo`))]|[numTargetRowsCopied -> 198493, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 4, numTargetRowsInserted -> 23, numTargetRowsUpdated -> 26, numOutputRows -> 198542, numSourceRows -> 44, numTargetFilesRemoved -> 6]|\n|10 |UPDATE   |[predicate -> (InvoiceNo#1171 = 561648)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 15, numCopiedRows -> 74953]                                                                                                                               |\n|9  |UPDATE   |[predicate -> (InvoiceNo#1171 = 536796)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 69, numCopiedRows -> 68563]                                                                                                                               |\n|8  |UPDATE   |[predicate -> (InvoiceNo#1171 = 557628)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 19, numCopiedRows -> 74949]                                                                                                                               |\n|7  |UPDATE   |[predicate -> (InvoiceNo#1171 = 569718)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 17, numCopiedRows -> 74951]                                                                                                                               |\n|6  |UPDATE   |[predicate -> (InvoiceNo#1171 = 537193)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 21, numCopiedRows -> 68611]                                                                                                                               |\n|5  |UPDATE   |[predicate -> (InvoiceNo#1171 = 569705)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 6, numCopiedRows -> 74962]                                                                                                                                |\n|4  |DELETE   |[predicate -> [\"(spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = 554485)\"]]                                                                                                                |[numRemovedFiles -> 2, numDeletedRows -> 2, numAddedFiles -> 1, numCopiedRows -> 37483]                                                                                                                                |\n|3  |UPDATE   |[predicate -> (InvoiceNo#2697 = 554485)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 37483]                                                                                                                                |\n|2  |WRITE    |[mode -> Append, partitionBy -> []]                                                                                                                                                               |[numFiles -> 1, numOutputBytes -> 2369, numOutputRows -> 1]                                                                                                                                                            |\n|1  |WRITE    |[mode -> Append, partitionBy -> []]                                                                                                                                                               |[numFiles -> 3, numOutputBytes -> 1082711, numOutputRows -> 99463]                                                                                                                                                     |\n|0  |WRITE    |[mode -> Overwrite, partitionBy -> []]                                                                                                                                                            |[numFiles -> 3, numOutputBytes -> 1083137, numOutputRows -> 99057]                                                                                                                                                     |\n+---+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"
    }
   ],
   "source": [
    "print(\"####### HISTORY ########\")\n",
    "\n",
    "# Observe history of actions taken on a Delta table\n",
    "history = deltaTable.history().select('version','operation', 'operationParameters', \\\n",
    "                                      'operationMetrics') \\\n",
    "                              .withColumnRenamed(\"version\", \"ver\")\n",
    "\n",
    "history.show(100, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Vacuum - Data Retention\n",
    "\n",
    "delta.logRetentionDuration - default 30 days\n",
    "\n",
    "delta.deletedFileRetentionDuration - default 30 days\n",
    "\n",
    "    - Don't need to set them to be the same. You may want to keep the log files around after the tombstoned files are purged.\n",
    "    - Time travel in order of months/years infeasible\n",
    "    - Initially desinged to correct mistakes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are ['1033'] files.\n"
    }
   ],
   "source": [
    "# files_in_dir(deltaPath,\"parquet\")\n",
    "# Count files in deltaPath\n",
    "\n",
    "numFiles = ! ls $deltaPath/*parquet | wc -l\n",
    "print(f\"There are {numFiles} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       ",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-cac1ced07679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#spark.sql(\"vacuum SalesDeltaFormat retain 0 hours dry run\").show(100, truncate = False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"VACUUM SalesDeltaFormat RETAIN 0 HOURS dry run\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \"\"\"\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       "
     ]
    }
   ],
   "source": [
    "# Attempt to vacuum table with default settings\n",
    "\n",
    "#spark.sql(\"vacuum SalesDeltaFormat retain 0 hours dry run\").show(100, truncate = False)\n",
    "spark.sql(\"VACUUM SalesDeltaFormat RETAIN 0 HOURS dry run\").show(100, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----------------------------------------------------------------------------------+\n|path                                                                               |\n+-----------------------------------------------------------------------------------+\n|file:/home/oliver/DeltaLake/training/online_retail_dataset/delta/online_retail_data|\n+-----------------------------------------------------------------------------------+\n\nCPU times: user 2.75 ms, sys: 1.43 ms, total: 4.18 ms\nWall time: 23.9 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "# Vacuum Delta table to remove all history\n",
    "\n",
    "spark.sql(\"VACUUM SalesDeltaFormat RETAIN 0 HOURS\").show(truncate = False)\n",
    "# ! Can use deltaTable.vacuum(0) against directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "279KB    2020-11-11 16:13:39  part-00002-f14f22a5-b50a-41bd-85a5-d66be4f819e4-c000.snappy.parquet\n379KB    2020-11-11 16:13:39  part-00001-6cf1a758-7c65-4b52-a884-78f77291b0d8-c000.snappy.parquet\n399KB    2020-11-11 16:13:39  part-00000-9d4068ec-b5a3-458d-982a-53bf31f53e15-c000.snappy.parquet\n\nNumber of file/s: 3 | Total size: 1.7M\n"
    }
   ],
   "source": [
    "files_in_dir(deltaPath,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2KB      2020-11-11 16:09:30  00000000000000000000.json\n765B     2020-11-11 16:09:59  00000000000000000001.json\n410B     2020-11-11 16:10:31  00000000000000000002.json\n902B     2020-11-11 16:10:53  00000000000000000003.json\n775B     2020-11-11 16:11:17  00000000000000000004.json\n904B     2020-11-11 16:11:31  00000000000000000005.json\n905B     2020-11-11 16:11:33  00000000000000000006.json\n905B     2020-11-11 16:11:34  00000000000000000007.json\n905B     2020-11-11 16:11:36  00000000000000000008.json\n905B     2020-11-11 16:11:37  00000000000000000009.json\n905B     2020-11-11 16:11:39  00000000000000000010.json\n17KB     2020-11-11 16:11:40  00000000000000000010.checkpoint.parquet\n2KB      2020-11-11 16:12:08  00000000000000000011.json\n169KB    2020-11-11 16:13:02  00000000000000000012.json\n141KB    2020-11-11 16:13:13  00000000000000000013.json\n1KB      2020-11-11 16:13:40  00000000000000000014.json\n\nNumber of file/s: 16 | Total size: 400K\n"
    }
   ],
   "source": [
    "files_in_dir(deltaLogPath,\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(add=None, commitInfo=Row(isBlindAppend=False, operation='WRITE', operationMetrics=Row(numFiles='3', numOutputBytes='1083137', numOutputRows='99057'), operationParameters=Row(mode='Overwrite', partitionBy='[]'), readVersion=13, timestamp=1605132820024, userMetadata='11-11-2020 Data Merge Message;'), remove=None),\n Row(add=Row(dataChange=True, modificationTime=1605132819000, path='part-00000-9d4068ec-b5a3-458d-982a-53bf31f53e15-c000.snappy.parquet', size=408716), commitInfo=None, remove=None),\n Row(add=Row(dataChange=True, modificationTime=1605132819000, path='part-00001-6cf1a758-7c65-4b52-a884-78f77291b0d8-c000.snappy.parquet', size=388268), commitInfo=None, remove=None),\n Row(add=Row(dataChange=True, modificationTime=1605132819000, path='part-00002-f14f22a5-b50a-41bd-85a5-d66be4f819e4-c000.snappy.parquet', size=286153), commitInfo=None, remove=None),\n Row(add=None, commitInfo=None, remove=Row(dataChange=True, deletionTimestamp=1605132820024, path='part-00002-7e9952fb-7b6e-47a3-97e9-0718fbf17e1d-c000.snappy.parquet')),\n Row(add=None, commitInfo=None, remove=Row(dataChange=True, deletionTimestamp=1605132820024, path='part-00000-eb2ac4c8-361e-4d9d-b7c2-5dda3c50f92b-c000.snappy.parquet')),\n Row(add=None, commitInfo=None, remove=Row(dataChange=True, deletionTimestamp=1605132820024, path='part-00001-cae03ab2-174f-475a-954d-e480f62732d9-c000.snappy.parquet')),\n Row(add=None, commitInfo=None, remove=Row(dataChange=True, deletionTimestamp=1605132820024, path='part-00003-f6f97010-bb1d-4b47-b516-b4b243a1225b-c000.snappy.parquet'))]"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(deltaLogPath + \"/00000000000000000014.json\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "99057"
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", currentVersion).load(deltaPath).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[]"
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "# Configure Delta table to keep around 7 days of deleted data and 7 days of older log files\n",
    "\n",
    "spark.sql(\"ALTER TABLE SalesDeltaFormat SET TBLPROPERTIES ('delta.logRetentionDuration' = 'INTERVAL 7 DAYS', 'delta.deletedFileRetentionDuration' = 'INTERVAL 7 DAYS')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----------------------------+-----------------------------------------------------------------------------------------------+-------+\n|col_name                    |data_type                                                                                      |comment|\n+----------------------------+-----------------------------------------------------------------------------------------------+-------+\n|InvoiceNo                   |int                                                                                            |       |\n|StockCode                   |string                                                                                         |       |\n|Description                 |string                                                                                         |       |\n|Quantity                    |int                                                                                            |       |\n|InvoiceDate                 |string                                                                                         |       |\n|UnitPrice                   |double                                                                                         |       |\n|CustomerID                  |int                                                                                            |       |\n|Country                     |string                                                                                         |       |\n|                            |                                                                                               |       |\n|# Partitioning              |                                                                                               |       |\n|Not partitioned             |                                                                                               |       |\n|                            |                                                                                               |       |\n|# Detailed Table Information|                                                                                               |       |\n|Name                        |deltademo.salesdeltaformat                                                                     |       |\n|Location                    |file:/home/oliver/DeltaLake/training/online_retail_dataset/delta/online_retail_data            |       |\n|Provider                    |delta                                                                                          |       |\n|Table Properties            |[delta.deletedFileRetentionDuration=INTERVAL 7 DAYS,delta.logRetentionDuration=INTERVAL 7 DAYS]|       |\n+----------------------------+-----------------------------------------------------------------------------------------------+-------+\n\n"
    }
   ],
   "source": [
    "# Verify our changes\n",
    "spark.sql(\"describe extended SalesDeltaFormat\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+\n|namespace|\n+---------+\n|  default|\n|deltademo|\n+---------+\n\n+------------------+\n|current_database()|\n+------------------+\n|         deltademo|\n+------------------+\n\n+-------------------------+-------------------------------------------------------------------+\n|database_description_item|database_description_value                                         |\n+-------------------------+-------------------------------------------------------------------+\n|Database Name            |deltademo                                                          |\n|Comment                  |Esta es una DB demo para uso del formato Delta                     |\n|Location                 |file:/home/oliver/DeltaLake/training/online_retail_dataset/DATABASE|\n|Owner                    |oliver                                                             |\n+-------------------------+-------------------------------------------------------------------+\n\n+---------+------------------+-----------+\n|database |tableName         |isTemporary|\n+---------+------------------+-----------+\n|deltademo|salesdeltaformat  |false      |\n|deltademo|salesparquetformat|false      |\n|deltademo|tbl_checkpointfile|false      |\n+---------+------------------+-----------+\n\n+----------------------------+--------------------------------------------------------------------------------------+-------+\n|col_name                    |data_type                                                                             |comment|\n+----------------------------+--------------------------------------------------------------------------------------+-------+\n|Action                      |string                                                                                |null   |\n|filename                    |string                                                                                |null   |\n|ActionDate                  |bigint                                                                                |null   |\n|                            |                                                                                      |       |\n|# Detailed Table Information|                                                                                      |       |\n|Database                    |deltademo                                                                             |       |\n|Table                       |tbl_checkpointfile                                                                    |       |\n|Owner                       |oliver                                                                                |       |\n|Created Time                |Wed Nov 11 16:11:44 CST 2020                                                          |       |\n|Last Access                 |UNKNOWN                                                                               |       |\n|Created By                  |Spark 3.0.0                                                                           |       |\n|Type                        |MANAGED                                                                               |       |\n|Provider                    |hive                                                                                  |       |\n|Table Properties            |[transient_lastDdlTime=1605132718]                                                    |       |\n|Statistics                  |1940 bytes                                                                            |       |\n|Location                    |file:/home/oliver/DeltaLake/training/online_retail_dataset/DATABASE/tbl_checkpointfile|       |\n|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                                    |       |\n|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                                              |       |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat                            |       |\n|Storage Properties          |[serialization.format=1]                                                              |       |\n+----------------------------+--------------------------------------------------------------------------------------+-------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "#spark.stop()\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "# Current DB should be deltademo\n",
    "spark.sql(\"USE deltademo\")\n",
    "spark.sql(\"SELECT CURRENT_DATABASE()\").show()\n",
    "spark.sql(\"DESCRIBE DATABASE deltademo\").show(truncate = False)\n",
    "spark.sql(\"SHOW TABLES\").show(truncate= False)\n",
    "spark.sql(\"DESCRIBE EXTENDED tbl_checkpointfile\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------+-------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+--------------------+\n|version|          timestamp|userId|userName|        operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics|        userMetadata|\n+-------+-------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+--------------------+\n|     15|2020-11-11 16:15:01|  null|    null|SET TBLPROPERTIES|[properties -> {\"...|null|    null|     null|         14|          null|         true|              []|11-11-2020 Data M...|\n+-------+-------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+--------------------+\n\n"
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE HISTORY SalesDeltaFormat LIMIT 1\").show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}